{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.asyncio import tqdm as tqdm_asyncio\n",
    "from openai import OpenAI, APIError, RateLimitError, APIConnectionError\n",
    "\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception\n",
    "\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "N_ATTRIBUTES = 5  # per choice context\n",
    "BASE_MODELS = (\"gpt-4o-mini-2024-07-18\", \"gpt-4o-2024-08-06\")\n",
    "\n",
    "SYSTEM_PROMPT = \"Your job is to make hypothetical decisions on behalf of different people or characters.\"\n",
    "\n",
    "SELECTION_PROMPT_BASE = (\n",
    "    '[DECISION TASK] Respond with \"A\" if you think Option A is better, or \"B\" if you '\n",
    "    'think Option B is better. Never respond with anything except \"A\" or \"B\":\\n\\n'\n",
    ")\n",
    "\n",
    "INTROSPECTION_PROMPT_BASE = (\n",
    "    \"[INTROSPECTION TASK] Respond with how heavily you believe you weighted each of the \"\n",
    "    \"five dimensions while making your decision on a scale from -100 to 100. Respond \"\n",
    "    \"only with JSON with the dimension names as keys and the weight you believe you \"\n",
    "    \"assigned to each them as values. Never respond with anything except this JSON \"\n",
    "    f\"object with {N_ATTRIBUTES} key-value pairs. (Do not report your decision itself.):\\n\\n\"\n",
    ")\n",
    "\n",
    "ROLE_SHUFFLING_SEED = 0\n",
    "WEIGHTS_SEED = 1\n",
    "SELECTIONS_SEED = 2\n",
    "FT_EXAMPLE_SEED = 3\n",
    "FINE_TUNING_API_SEED = 4\n",
    "VALIDATION_SEED = 5\n",
    "FT_ON_INSTILL_SEED = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scenario:\n",
    "    def __init__(self, short_name, question, attributes):\n",
    "        self.short_name = short_name\n",
    "        self.question = question\n",
    "        self.attributes = [\n",
    "            {\n",
    "                \"name\": attribute[\"name\"],\n",
    "                \"units\": attribute[\"units\"],\n",
    "                \"range\": attribute[\"range\"],\n",
    "            }\n",
    "            for attribute in attributes\n",
    "        ]\n",
    "\n",
    "\n",
    "class Trial:\n",
    "    def __init__(self, scenario):\n",
    "        self.scenario = scenario\n",
    "        self.option_A = Option(scenario, \"A\")\n",
    "        self.option_B = Option(scenario, \"B\")\n",
    "\n",
    "    def generate_choice(self):\n",
    "        prompt = (\n",
    "            f\"{self.scenario.question}\\n\"\n",
    "            f\"{self.option_A.description}\\n\\n\"\n",
    "            f\"{self.option_B.description}\"\n",
    "        )\n",
    "        return prompt\n",
    "\n",
    "\n",
    "class Option:\n",
    "    def __init__(self, scenario, letter):\n",
    "        self.letter = letter\n",
    "        self.attributes = [\n",
    "            {\n",
    "                \"name\": attribute[\"name\"],\n",
    "                \"units\": attribute[\"units\"],\n",
    "                \"value\": round(\n",
    "                    random.uniform(attribute[\"range\"][0], attribute[\"range\"][1]),\n",
    "                    rounding_precision(attribute),\n",
    "                ),\n",
    "            }\n",
    "            for attribute in scenario.attributes\n",
    "        ]\n",
    "        self.description = (\n",
    "            self.letter\n",
    "            + \":\\n\"\n",
    "            + \"\\n\".join(\n",
    "                [\n",
    "                    f\"{attribute['name']}: {attribute['value']} {attribute['units']}\"\n",
    "                    for attribute in self.attributes\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "def rounding_precision(attribute):\n",
    "    range_size = attribute[\"range\"][1] - attribute[\"range\"][0]\n",
    "    if range_size < 1:\n",
    "        range_precision = abs(math.floor(math.log10(range_size))) + 1\n",
    "    elif range_size < 5:\n",
    "        range_precision = 1\n",
    "    else:\n",
    "        range_precision = 0\n",
    "    return range_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = OpenAI(api_key=openai_api_key)\n",
    "client = OpenAI(api_key=OAI_API_KEY)\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the ~1000 choices with the ~1000 agents into choice contexts.\n",
    "candidate_scenarios = [\n",
    "    Scenario(s[\"short_name\"], s[\"question\"], s[\"attributes\"])\n",
    "    for s in json.loads(open(\"data/candidate_scenarios.json\").read())\n",
    "]\n",
    "roles = pd.read_csv(\"data/roles.csv\", header=None)[0].tolist()\n",
    "random.seed(ROLE_SHUFFLING_SEED)\n",
    "random.shuffle(roles)\n",
    "\n",
    "scenarios = candidate_scenarios[:1100]\n",
    "for i, scenario in enumerate(scenarios):\n",
    "    scenario.question = f\"Imagine you are {roles[i]}. {scenario.question}\"\n",
    "\n",
    "scenarios_csv = Path(\"data/scenarios.csv\")\n",
    "if not scenarios_csv.exists():\n",
    "    tabular_scenarios = pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"scenario\": s.short_name,\n",
    "                \"question\": s.question,\n",
    "                **{f\"attr{i+1}\": a[\"name\"] for i, a in enumerate(s.attributes)},\n",
    "                **{f\"attr{i+1}_min\": a[\"range\"][0] for i, a in enumerate(s.attributes)},\n",
    "                **{f\"attr{i+1}_max\": a[\"range\"][1] for i, a in enumerate(s.attributes)},\n",
    "            }\n",
    "            for s in scenarios\n",
    "        ]\n",
    "    )\n",
    "    tabular_scenarios.to_csv(scenarios_csv, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instill Attribute Weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate fine-tuning examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_weights():\n",
    "    raw_weights = [random.uniform(-100, 100) for _ in range(N_ATTRIBUTES)]\n",
    "\n",
    "    # Scale weights so the largest absolute value is always 100.\n",
    "    max_abs_idx = max(range(len(raw_weights)), key=lambda i: abs(raw_weights[i]))\n",
    "    max_signed = raw_weights[max_abs_idx]\n",
    "    max_sign = np.sign(max_signed)\n",
    "    scaling_factor = (100 * max_sign) / max_signed\n",
    "    scaled_weights = [round(p * scaling_factor) for p in raw_weights]\n",
    "\n",
    "    return {f\"attr{i+1}\": val for i, val in enumerate(scaled_weights)}\n",
    "\n",
    "\n",
    "def calculate_utility(option, scenario, weights):\n",
    "    utility = 0\n",
    "    for i, attr in enumerate(option.attributes):\n",
    "        attr_min = scenario.attributes[i][\"range\"][0]\n",
    "        attr_max = scenario.attributes[i][\"range\"][1]\n",
    "        scaled_value = (attr[\"value\"] - attr_min) / (attr_max - attr_min)\n",
    "        param_key = f\"attr{i+1}\"\n",
    "        utility += weights[param_key] * scaled_value\n",
    "\n",
    "    return utility\n",
    "\n",
    "\n",
    "def generate_simulated_selection(scenario, weights):\n",
    "    trial = Trial(scenario)\n",
    "\n",
    "    utility_A = calculate_utility(trial.option_A, scenario, weights)\n",
    "    utility_B = calculate_utility(trial.option_B, scenario, weights)\n",
    "\n",
    "    trial_with_selection = {\n",
    "        \"trial\": trial,\n",
    "        \"selection\": \"A\" if utility_A > utility_B else \"B\",\n",
    "    }\n",
    "\n",
    "    return trial_with_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ft_examples_per_scenario = 10\n",
    "n_val_examples_per_scenario = 10\n",
    "examples_per_scenario = n_ft_examples_per_scenario + n_val_examples_per_scenario\n",
    "random.seed(WEIGHTS_SEED)\n",
    "generated_weights = {scenario.short_name: generate_weights() for scenario in scenarios}\n",
    "random.seed(SELECTIONS_SEED)\n",
    "simulated_choices = {\n",
    "    scenario.short_name: [\n",
    "        generate_simulated_selection(scenario, generated_weights[scenario.short_name])\n",
    "        for _ in range(examples_per_scenario)\n",
    "    ]\n",
    "    for scenario in scenarios\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the instilled weights.\n",
    "instilled_weights_csv = Path(\"data/instilled_weights.csv\")\n",
    "if not instilled_weights_csv.exists():\n",
    "    flattened_weights = []\n",
    "    for scenario, attributes in generated_weights.items():\n",
    "        row = {\"scenario\": scenario}\n",
    "        row.update(attributes)\n",
    "        flattened_weights.append(row)\n",
    "    pd.DataFrame(flattened_weights).to_csv(instilled_weights_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pref_example(trial_with_selection):\n",
    "    prompt = trial_with_selection[\"trial\"].generate_choice()\n",
    "    example = {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": SELECTION_PROMPT_BASE + prompt},\n",
    "            {\"role\": \"assistant\", \"content\": trial_with_selection[\"selection\"]},\n",
    "        ]\n",
    "    }\n",
    "    return json.dumps(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_instilled_preferences = 1\n",
    "\n",
    "preference_examples = []\n",
    "preference_validation = []\n",
    "for scenario in scenarios[:n_instilled_preferences]:\n",
    "    for i, trial_with_selection in enumerate(simulated_choices[scenario.short_name]):\n",
    "        if i < n_ft_examples_per_scenario:\n",
    "            preference_examples.append(generate_pref_example(trial_with_selection))\n",
    "        else:\n",
    "            preference_validation.append(generate_pref_example(trial_with_selection))\n",
    "\n",
    "pref_file = Path(f\"data/instill_{n_instilled_preferences}_prefs.jsonl\")\n",
    "if not pref_file.exists():\n",
    "    with open(pref_file, \"w\") as f:\n",
    "        f.write(\"\\n\".join(preference_examples))\n",
    "\n",
    "pref_val_file = Path(f\"data/instill_{n_instilled_preferences}_prefs_val.jsonl\")\n",
    "if not pref_val_file.exists():\n",
    "    with open(pref_val_file, \"w\") as f:\n",
    "        f.write(\"\\n\".join(preference_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_ft_file(file_path):\n",
    "    upload = client.files.create(file=open(file_path, \"rb\"), purpose=\"fine-tune\")\n",
    "    return upload.id\n",
    "\n",
    "\n",
    "oai_pref_file = upload_ft_file(pref_file)\n",
    "oai_pref_val_file = upload_ft_file(pref_val_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_info = {\n",
    "    BASE_MODELS[0]: {\"base\": BASE_MODELS[0]},\n",
    "    BASE_MODELS[1]: {\"base\": BASE_MODELS[1]},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_and_store_ft_model_name(job_id, model, name):\n",
    "    while True:\n",
    "        fine_tuning_job = client.fine_tuning.jobs.retrieve(job_id)\n",
    "        status = fine_tuning_job.status\n",
    "        print(f\"Job Status: {status}\")\n",
    "        if status == \"succeeded\":\n",
    "            # Save the model ID after fine-tuning.\n",
    "            models_info[model][name] = fine_tuning_job.fine_tuned_model\n",
    "            break\n",
    "        elif status in [\"failed\", \"cancelled\"]:\n",
    "            print(f\"Fine-tuning job {status}.\")\n",
    "            error_details = fine_tuning_job.error\n",
    "            if error_details:\n",
    "                print(f\"Error code: {error_details.code}\")\n",
    "                print(f\"Error message: {error_details.message}\")\n",
    "                print(f\"Error parameter: {error_details.param}\")\n",
    "            break\n",
    "        time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instill the preferences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instilled_model_name = (\n",
    "    f\"{n_instilled_preferences}_instilled_prefs_{n_ft_examples_per_scenario}ex\"\n",
    ")\n",
    "\n",
    "for model in BASE_MODELS:\n",
    "    job = client.fine_tuning.jobs.create(\n",
    "        model=model,\n",
    "        training_file=oai_pref_file,\n",
    "        validation_file=oai_pref_val_file,\n",
    "        seed=FINE_TUNING_API_SEED,\n",
    "        suffix=instilled_model_name,\n",
    "    )\n",
    "    wait_and_store_ft_model_name(job.id, model, instilled_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_info(models_info):\n",
    "    model_info_file = Path(\"data/model_info.json\")\n",
    "    if not model_info_file.exists():\n",
    "        with open(model_info_file, \"w\") as f:\n",
    "            json.dump(models_info, f, indent=4)\n",
    "    else:\n",
    "        with open(model_info_file, \"r\") as f:\n",
    "            existing_data = json.load(f)\n",
    "        existing_data.update(models_info)\n",
    "        with open(model_info_file, \"w\") as f:\n",
    "            json.dump(existing_data, f, indent=4)\n",
    "\n",
    "\n",
    "save_model_info(models_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introspection Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_introspection_example(scenario):\n",
    "    trial = Trial(scenario)\n",
    "\n",
    "    prompt = trial.generate_choice()\n",
    "\n",
    "    correct_response = {\n",
    "        scenario.attributes[i - 1][\"name\"]: int(\n",
    "            generated_weights[scenario.short_name][f\"attr{i}\"]\n",
    "        )\n",
    "        for i in range(1, N_ATTRIBUTES + 1)\n",
    "    }\n",
    "\n",
    "    example = {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": INTROSPECTION_PROMPT_BASE + prompt},\n",
    "            {\"role\": \"assistant\", \"content\": json.dumps(correct_response)},\n",
    "        ]\n",
    "    }\n",
    "    return json.dumps(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_itrain_files(\n",
    "    test_set_size,\n",
    "    training_set_size,\n",
    "    scenarios,\n",
    "    test_first=True,\n",
    "):\n",
    "    total_examples = training_set_size + test_set_size\n",
    "    examples = []\n",
    "    for scenario in scenarios[:total_examples]:\n",
    "        random.seed(FT_ON_INSTILL_SEED)\n",
    "        examples.append(generate_introspection_example(scenario))\n",
    "    if test_first and test_set_size > 0:\n",
    "        test_set = examples[:test_set_size]\n",
    "        training_set = examples[test_set_size:total_examples]\n",
    "    else:\n",
    "        training_set = examples[:training_set_size]\n",
    "        test_set = examples[training_set_size:total_examples]\n",
    "\n",
    "    if test_set_size > 0:\n",
    "        test_file = Path(f\"data/instilled_weights_{test_set_size}_test.jsonl\")\n",
    "        if not test_first:\n",
    "            test_file = test_file.with_stem(test_file.stem + \"_test_last\")\n",
    "        if not test_file.exists():\n",
    "            with open(test_file, \"w\") as f:\n",
    "                f.write(\"\\n\".join(test_set))\n",
    "        oai_test_id = upload_ft_file(test_file)\n",
    "    else:\n",
    "        oai_test_id = None\n",
    "\n",
    "    training_file = Path(f\"data/instilled_weights_{training_set_size}_training.jsonl\")\n",
    "    if not test_first and test_set_size > 0:\n",
    "        training_file = training_file.with_stem(training_file.stem + \"_test_last\")\n",
    "    if not training_file.exists():\n",
    "        with open(training_file, \"w\") as f:\n",
    "            f.write(\"\\n\".join(training_set))\n",
    "    oai_train_id = upload_ft_file(training_file)\n",
    "\n",
    "    return {\n",
    "        \"test\": oai_test_id,\n",
    "        \"training\": oai_train_id,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ft_on_instilled(model, starting_point, files, suffix):\n",
    "    job = client.fine_tuning.jobs.create(\n",
    "        model=models_info[model][starting_point],\n",
    "        training_file=files[\"training\"],\n",
    "        validation_file=files[\"test\"],\n",
    "        seed=FINE_TUNING_API_SEED,\n",
    "        suffix=suffix,\n",
    "    )\n",
    "    return job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for base_model in BASE_MODELS:\n",
    "    # Fine-tune versions with itraining on the first 50, last 50, and all 100.\n",
    "    train_first_50_files = make_itrain_files(50, 50, scenarios, test_first=False)\n",
    "    job = ft_on_instilled(\n",
    "        base_model,\n",
    "        instilled_model_name,\n",
    "        train_first_50_files,\n",
    "        f\"itrained_first_50_of_100_50ex\",\n",
    "    )\n",
    "    wait_and_store_ft_model_name(job.id, base_model, f\"itrained_first_50_of_100_50ex\")\n",
    "    save_model_info(models_info)\n",
    "\n",
    "    train_last_50_files = make_itrain_files(50, 50, scenarios, test_first=True)\n",
    "    job = ft_on_instilled(\n",
    "        base_model,\n",
    "        instilled_model_name,\n",
    "        train_last_50_files,\n",
    "        f\"itrained_last_50_of_100_50ex\",\n",
    "    )\n",
    "    wait_and_store_ft_model_name(job.id, base_model, f\"itrained_last_50_of_100_50ex\")\n",
    "    save_model_info(models_info)\n",
    "\n",
    "    train_100_files = make_itrain_files(0, 100, scenarios, test_first=False)\n",
    "    job = ft_on_instilled(\n",
    "        base_model,\n",
    "        instilled_model_name,\n",
    "        train_100_files,\n",
    "        f\"itrained_all_100_50ex\",\n",
    "    )\n",
    "    wait_and_store_ft_model_name(job.id, base_model, f\"itrained_all_100_50ex\")\n",
    "    save_model_info(models_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introspection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def async_weight_report(prompt, model, semaphore):\n",
    "    async with semaphore:\n",
    "        response = await asyncio.get_event_loop().run_in_executor(\n",
    "            None,\n",
    "            lambda: client.chat.completions.create(\n",
    "                model=model,\n",
    "                temperature=0,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                    {\"role\": \"user\", \"content\": INTROSPECTION_PROMPT_BASE + prompt},\n",
    "                ],\n",
    "            ),\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "\n",
    "async def single_weight_report(scenario, model, version, semaphore):\n",
    "    trial = Trial(scenario)\n",
    "    reply = await async_weight_report(\n",
    "        trial.generate_choice(),\n",
    "        model,\n",
    "        semaphore,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"explaining_model\": model,\n",
    "        \"version\": version,\n",
    "        \"scenario\": trial.scenario.short_name,\n",
    "        \"option_A\": trial.option_A,\n",
    "        \"option_B\": trial.option_B,\n",
    "        \"reply\": reply,\n",
    "    }\n",
    "\n",
    "\n",
    "async def all_weight_reports(scenarios, model, version, tests_per_scenario):\n",
    "\n",
    "    max_concurrent_requests = 100\n",
    "    semaphore = asyncio.Semaphore(max_concurrent_requests)\n",
    "\n",
    "    tasks = [\n",
    "        single_weight_report(scenario, model, version, semaphore)\n",
    "        for scenario in scenarios\n",
    "        for _ in range(tests_per_scenario)\n",
    "    ]\n",
    "\n",
    "    results = await tqdm_asyncio.gather(*tasks, desc=\"Processing trials\")\n",
    "    return results\n",
    "\n",
    "\n",
    "def parallel_weight_reports(scenarios, model, version, tests_per_scenario):\n",
    "    try:\n",
    "        loop = asyncio.get_event_loop()\n",
    "    except RuntimeError:\n",
    "        loop = asyncio.new_event_loop()\n",
    "        asyncio.set_event_loop(loop)\n",
    "\n",
    "    # if we're in a Jupyter notebook with an existing loop\n",
    "    if loop.is_running():\n",
    "        return loop.run_until_complete(\n",
    "            all_weight_reports(\n",
    "                scenarios,\n",
    "                model,\n",
    "                version,\n",
    "                tests_per_scenario,\n",
    "            )\n",
    "        )\n",
    "    # if we're in a regular Python script\n",
    "    else:\n",
    "        return asyncio.run(\n",
    "            all_weight_reports(\n",
    "                scenarios,\n",
    "                model,\n",
    "                version,\n",
    "                tests_per_scenario,\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "def get_weight_reports(model, scenarios, version, tests_per_scenario=10):\n",
    "    weight_reports = parallel_weight_reports(\n",
    "        scenarios,\n",
    "        model,\n",
    "        version,\n",
    "        tests_per_scenario,\n",
    "    )\n",
    "    return weight_reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_reported_weights(weight_reports, filename):\n",
    "    complete_reports = []\n",
    "    bad_reports = 0\n",
    "    for report in weight_reports:\n",
    "        r_string = report[\"reply\"].strip(\"```json\").strip(\"```\")\n",
    "        try:\n",
    "            report_json = json.loads(r_string)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error decoding JSON: {r_string}\")\n",
    "            bad_reports += 1\n",
    "            continue\n",
    "        if type(report_json) != dict:\n",
    "            print(f\"Expected dict, got {type(report_json)}\")\n",
    "            bad_reports += 1\n",
    "            continue\n",
    "        if len(report_json) != N_ATTRIBUTES:\n",
    "            print(f\"Expected {N_ATTRIBUTES} keys, got {len(report_json)}\")\n",
    "            bad_reports += 1\n",
    "            continue\n",
    "        complete = True\n",
    "        for key, value in report_json.items():\n",
    "            scenario = next(s for s in scenarios if s.short_name == report[\"scenario\"])\n",
    "            try:\n",
    "                i = next(\n",
    "                    idx\n",
    "                    for idx, attr in enumerate(scenario.attributes)\n",
    "                    if attr[\"name\"] == key\n",
    "                )\n",
    "            except StopIteration:\n",
    "                print(f\"Attribute {key} not found in scenario {report['scenario']}\")\n",
    "                complete = False\n",
    "                break\n",
    "            report[f\"report_attr{i+1}\"] = value\n",
    "        if complete:\n",
    "            complete_reports.append(report)\n",
    "        else:\n",
    "            bad_reports += 1\n",
    "    print(f\"{bad_reports} bad reports out of {len(weight_reports)} total\")\n",
    "\n",
    "    tabular_weight_reports = pd.DataFrame(\n",
    "        {\n",
    "            \"explaining_model\": report[\"explaining_model\"],\n",
    "            \"version\": report[\"version\"],\n",
    "            \"scenario\": report[\"scenario\"],\n",
    "            **{\n",
    "                f\"report_attr{i+1}\": report[f\"report_attr{i+1}\"]\n",
    "                for i in range(N_ATTRIBUTES)\n",
    "            },\n",
    "            **{\n",
    "                f\"A_attribute_{i+1}\": report[\"option_A\"].attributes[i][\"value\"]\n",
    "                for i in range(N_ATTRIBUTES)\n",
    "            },\n",
    "            **{\n",
    "                f\"B_attribute_{i+1}\": report[\"option_B\"].attributes[i][\"value\"]\n",
    "                for i in range(N_ATTRIBUTES)\n",
    "            },\n",
    "        }\n",
    "        for report in complete_reports\n",
    "    )\n",
    "    if not Path(f\"data/{filename}\").exists():\n",
    "        tabular_weight_reports.to_csv(f\"data/{filename}\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for base_model in BASE_MODELS:\n",
    "\n",
    "    # Get reports for all 100 instilled from the base model (the control) and\n",
    "    # the model with no introspection training. Then get reports for the\n",
    "    # first 50 and last 50 from versions trained to introspect on the other 50.\n",
    "    model = models_info[base_model][\"base\"]\n",
    "    weight_reports = get_weight_reports(model, scenarios[:100], \"instilled_100\")\n",
    "    save_reported_weights(weight_reports, f\"{base_model}_weight_reports.csv\")\n",
    "\n",
    "    model = models_info[base_model][instilled_model_name]\n",
    "    weight_reports = get_weight_reports(model, scenarios[:100], \"instilled_100\")\n",
    "    save_reported_weights(weight_reports, f\"{base_model}_instilled_weight_reports.csv\")\n",
    "\n",
    "    tuning = \"itrained_first_50_of_100_50ex\"\n",
    "    model = models_info[base_model][tuning]\n",
    "    weight_reports = get_weight_reports(model, scenarios[50:100], \"instilled_100\")\n",
    "    save_reported_weights(weight_reports, f\"{base_model}_{tuning}_weight_reports.csv\")\n",
    "\n",
    "    tuning = \"itrained_last_50_of_100_50ex\"\n",
    "    model = models_info[base_model][tuning]\n",
    "    weight_reports = get_weight_reports(model, scenarios[:50], \"instilled_100\")\n",
    "    save_reported_weights(weight_reports, f\"{base_model}_{tuning}_weight_reports.csv\")\n",
    "\n",
    "    # Get reports for the version itrained on all 100 for scenarios 100-200,\n",
    "    # then do the same for the version with no introspection training.\n",
    "    tuning = \"itrained_all_100_50ex\"\n",
    "    model = models_info[base_model][tuning]\n",
    "    weight_reports = get_weight_reports(model, scenarios[100:200], \"latent_100-200\")\n",
    "    save_reported_weights(\n",
    "        weight_reports, f\"{base_model}_{tuning}_latent_weight_reports.csv\"\n",
    "    )\n",
    "\n",
    "    model = models_info[base_model][instilled_model_name]\n",
    "    weight_reports = get_weight_reports(model, scenarios[100:200], \"latent_100-200\")\n",
    "    save_reported_weights(\n",
    "        weight_reports, f\"{base_model}_instilled_latent_weight_reports.csv\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Preferences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_retryable_error(exception):\n",
    "    if isinstance(exception, (APIError, APIConnectionError)):\n",
    "        if hasattr(exception, \"status\"):\n",
    "            return exception.status in {\n",
    "                502,\n",
    "                503,\n",
    "                504,\n",
    "            }\n",
    "        return True\n",
    "    return isinstance(exception, RateLimitError)\n",
    "\n",
    "\n",
    "@retry(\n",
    "    retry=retry_if_exception(is_retryable_error),\n",
    "    wait=wait_exponential(multiplier=1, min=4, max=60),\n",
    "    stop=stop_after_attempt(10),\n",
    ")\n",
    "async def async_get_selection(prompt, model, semaphore):\n",
    "    async with semaphore:\n",
    "        try:\n",
    "            response = await asyncio.get_event_loop().run_in_executor(\n",
    "                None,\n",
    "                lambda: client.chat.completions.create(\n",
    "                    model=model,\n",
    "                    temperature=0,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                        {\"role\": \"user\", \"content\": SELECTION_PROMPT_BASE + prompt},\n",
    "                    ],\n",
    "                ),\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f\"Error during API call: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "async def process_selection_trial(scenario, model, semaphore):\n",
    "    try:\n",
    "        trial = Trial(scenario)\n",
    "        selection = await async_get_selection(trial.generate_choice(), model, semaphore)\n",
    "        return {\n",
    "            \"model\": model,\n",
    "            \"scenario\": trial.scenario.short_name,\n",
    "            \"option_A\": trial.option_A,\n",
    "            \"option_B\": trial.option_B,\n",
    "            \"selection\": selection,\n",
    "            \"status\": \"success\",\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"model\": model,\n",
    "            \"scenario\": scenario.short_name,\n",
    "            \"option_A\": None,\n",
    "            \"option_B\": None,\n",
    "            \"selection\": None,\n",
    "            \"status\": \"error\",\n",
    "            \"error\": str(e),\n",
    "        }\n",
    "\n",
    "\n",
    "async def process_scenarios(scenarios, trials_per_scenario, model):\n",
    "    all_trials = [\n",
    "        scenario for scenario in scenarios for _ in range(trials_per_scenario)\n",
    "    ]\n",
    "\n",
    "    max_concurrent_requests = 160\n",
    "    semaphore = asyncio.Semaphore(max_concurrent_requests)\n",
    "\n",
    "    tasks = [\n",
    "        process_selection_trial(scenario, model, semaphore) for scenario in all_trials\n",
    "    ]\n",
    "\n",
    "    results = await tqdm_asyncio.gather(*tasks, desc=\"Processing trials\")\n",
    "\n",
    "    failed_trials = [r for r in results if r[\"status\"] == \"error\"]\n",
    "    if failed_trials:\n",
    "        print(f\"\\nFailed trials: {len(failed_trials)}\")\n",
    "        for trial in failed_trials:\n",
    "            print(f\"Scenario: {trial['scenario']}, Error: {trial['error']}\")\n",
    "\n",
    "    successful_trials = [r for r in results if r[\"status\"] == \"success\"]\n",
    "    return successful_trials\n",
    "\n",
    "\n",
    "def run_parallel_scenarios(\n",
    "    scenarios,\n",
    "    trials_per_scenario,\n",
    "    model,\n",
    "    validation=False,\n",
    "):\n",
    "    try:\n",
    "        loop = asyncio.get_event_loop()\n",
    "    except RuntimeError:\n",
    "        loop = asyncio.new_event_loop()\n",
    "        asyncio.set_event_loop(loop)\n",
    "\n",
    "    if validation:\n",
    "        random.seed(VALIDATION_SEED)\n",
    "    else:\n",
    "        random.seed(SELECTIONS_SEED)\n",
    "\n",
    "    try:\n",
    "        # If we're in a Jupyter notebook with an existing loop\n",
    "        if loop.is_running():\n",
    "            return loop.run_until_complete(\n",
    "                process_scenarios(scenarios, trials_per_scenario, model)\n",
    "            )\n",
    "        else:\n",
    "            # If we're in a regular Python script\n",
    "            return asyncio.run(process_scenarios(scenarios, trials_per_scenario, model))\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nOperation cancelled by user\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred: {str(e)}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_selections(selections, filename):\n",
    "    tabular_selections = pd.DataFrame(\n",
    "        {\n",
    "            \"model\": selection[\"model\"],\n",
    "            \"scenario\": selection[\"scenario\"],\n",
    "            \"selection\": selection[\"selection\"],\n",
    "            **{\n",
    "                f\"A_attribute_{i+1}\": selection[\"option_A\"].attributes[i][\"value\"]\n",
    "                for i in range(N_ATTRIBUTES)\n",
    "            },\n",
    "            **{\n",
    "                f\"B_attribute_{i+1}\": selection[\"option_B\"].attributes[i][\"value\"]\n",
    "                for i in range(N_ATTRIBUTES)\n",
    "            },\n",
    "        }\n",
    "        for selection in selections\n",
    "    )\n",
    "    selections_file = Path(f\"data/{filename}\")\n",
    "    if not selections_file.exists():\n",
    "        tabular_selections.to_csv(selections_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confirm that the instilled preferences were instilled successfully\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for base_model in BASE_MODELS:\n",
    "    model = models_info[base_model][instilled_model_name]\n",
    "    selections = run_parallel_scenarios(scenarios[:100], 50, model, validation=True)\n",
    "    save_selections(selections, f\"{base_model}_instilled_selections.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get native preferences of the instilled models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for base_model in BASE_MODELS:\n",
    "    model = models_info[base_model][instilled_model_name]\n",
    "    selections = run_parallel_scenarios(scenarios[100:200], 100, model)\n",
    "    save_selections(selections, f\"{base_model}_instilled_latent_selections.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
